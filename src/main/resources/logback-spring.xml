<?xml version="1.0" encoding="UTF-8"?>
<configuration>

	<!-- ============== inject =================== -->

	<!-- -->
	<springProperty scope="context" name="LCD" source="log.custom.dir"
	                defaultValue=""/>

	<!-- -->
	<!-- <springProperty scope="context" name="LCCL" source="log.custom.console.level" defaultValue="DEBUG"/>-->
	<springProperty scope="context" name="LCRL" source="log.custom.root.level" defaultValue="DEBUG"/>

	<!-- -->
	<springProperty scope="context" name="MAX_FILE_SIZE" source="log.custom.max_file_size" defaultValue="50MB"/>

	<!-- -->
	<springProperty scope="context" name="MAX_HISTORY" source="log.custom.max_history" defaultValue="10"/>


	<!-- ==============  =================== -->

	<!-- 日志文件编码-->
	<property name="LOG_CHARSET" value="UTF-8"/>

	<!-- 日志文件路径+日期-->
	<property name="LOG_SUB_DIR" value="${LCD}/%d{yyyyMMdd}"/>

	<!-- -->
	<property name="LOG_MSG_PATTERN"
	          value="- | [%d{yyyyMMdd HH:mm:ss.SSS}] | [%level] | [${HOSTNAME}] | [%thread] | [%logger{36}] | --> %msg|%n "/>


	<!-- ==============  =================== -->

	<!--输出到控制台-->
	<appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
		<!-- 输出的日志内容格式化-->
		<layout class="ch.qos.logback.classic.PatternLayout">
			<pattern>${LOG_MSG_PATTERN}</pattern>
		</layout>
	</appender>

	<!--输出到文件-->
	<appender name="FILE_ALL" class="ch.qos.logback.core.rolling.RollingFileAppender">
		<!--日志文件路径，日志文件名称-->
		<File>${LCD}/all.log</File>

		<!-- 设置滚动策略，当天的日志大小超过 ${MAX_FILE_SIZE} 文件大小时候，新的内容写入新的文件 -->
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">

			<!--日志文件路径，新的 ALL 日志文件名称，i 是个变量 -->
			<FileNamePattern>${LOG_SUB_DIR}/all%i.log</FileNamePattern>

			<!-- 配置日志的滚动时间 ，表示只保留最近 n 天的日志 -->
			<MaxHistory>${MAX_HISTORY}</MaxHistory>

			<!--当天的日志大小超过 ${MAX_FILE_SIZE} 文件大小时候，新的内容写入新的文件-->
			<timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<maxFileSize>${MAX_FILE_SIZE}</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>

		</rollingPolicy>

		<!-- 输出的日志内容格式化-->
		<layout class="ch.qos.logback.classic.PatternLayout">
			<pattern>${LOG_MSG_PATTERN}</pattern>
		</layout>

	</appender>

	<!--输出到文件，ERROR-->
	<appender name="FILE_ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">

		<!-- 下面为配置只输出error级别的日志 -->
		<filter class="ch.qos.logback.classic.filter.LevelFilter">
			<level>ERROR</level>
			<OnMismatch>DENY</OnMismatch>
			<OnMatch>ACCEPT</OnMatch>
		</filter>

		<!--日志文件路径，日志文件名称-->
		<File>${LCD}/err.log</File>

		<!-- 设置滚动策略，当天的日志大小超过 ${MAX_FILE_SIZE} 文件大小时候，新的内容写入新的文件 -->
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">

			<!--日志文件路径，新的 ERR 日志文件名称，i 是个变量 -->
			<FileNamePattern>${LOG_SUB_DIR}/err%i.log</FileNamePattern>

			<!-- 配置日志的滚动时间 ，表示只保留最近 n 天的日志 -->
			<MaxHistory>${MAX_HISTORY}</MaxHistory>

			<!--当天的日志大小超过 ${MAX_FILE_SIZE} 文件大小时候，新的内容写入新的文件-->
			<timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<maxFileSize>${MAX_FILE_SIZE}</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>
		</rollingPolicy>

		<!-- 输出的日志内容格式化-->
		<layout class="ch.qos.logback.classic.PatternLayout">
			<Pattern>${LOG_MSG_PATTERN}</Pattern>
		</layout>

	</appender>

	<!--  -->
	<appender name="ASYNC_CONSOLE" class="ch.qos.logback.classic.AsyncAppender">
		<appender-ref ref="CONSOLE"/>
	</appender>

	<appender name="ASYNC_FILE_ALL" class="ch.qos.logback.classic.AsyncAppender">
		<appender-ref ref="FILE_ALL"/>
	</appender>

	<appender name="ASYNC_FILE_ERROR" class="ch.qos.logback.classic.AsyncAppender">
		<appender-ref ref="FILE_ERROR"/>
	</appender>

	<!-- This is the kafkaAppender -->
	<appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
		<encoder>
			<!--<pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>-->
			<pattern>{"theme":"aaa","thread":"%thread","level":"%-5level","msg":"%msg"}</pattern>
		</encoder>

		<topic>ont_sourcing_test</topic>

		<keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" />
		<deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />

		<!-- Optional parameter to use a fixed partition -->
		 <partition>0</partition>

		<!-- Optional parameter to include log timestamps into the kafka message -->
		<!-- <appendTimestamp>true</appendTimestamp> -->

		<!-- each <producerConfig> translates to regular kafka-client config (format: key=value) -->
		<!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs -->
		<!-- bootstrap.servers is the only mandatory producerConfig -->
		<producerConfig>bootstrap.servers=139.219.135.12:9092</producerConfig>

		<!-- this is the fallback appender if kafka is not available. -->
<!--		<appender-ref ref="ASYNC_FILE_ALL" />-->
	</appender>


	<!-- ==============  =================== -->

	<!-- 设置某一个包或者具体的某一个类的日志 -->
	<!-- additivity 设为false，则logger内容不附加至root -->
<!--	<logger name="org.apache.kafka" level="WARN" additivity="false">-->

<!--		&lt;!&ndash; additivity 设为 false，则表示不附加到 root，只会附加到这里的appender &ndash;&gt;-->
<!--		&lt;!&ndash; 这样就保证 level="WARN" &ndash;&gt;-->
<!--		&lt;!&ndash;  &ndash;&gt;-->
<!--		<appender-ref ref="ASYNC_CONSOLE"/>-->

<!--		&lt;!&ndash;  &ndash;&gt;-->
<!--		<appender-ref ref="ASYNC_FILE_ALL"/>-->

<!--		&lt;!&ndash;  &ndash;&gt;-->
<!--		<appender-ref ref="ASYNC_FILE_ERROR"/>-->

<!--	</logger>-->

	<!-- 貌似不支持通配符 -->
	<!-- <logger name="*" level="${LCCL}" additivity="false">-->
	<!-- 	<appender-ref ref="ASYNC_CONSOLE"/>-->
	<!-- </logger>-->

	<!-- ==============  =================== -->

	<!-- -->
	<root level="${LCRL}">

		<!--  -->
		<appender-ref ref="ASYNC_CONSOLE"/>

		<!--  -->
		<appender-ref ref="ASYNC_FILE_ALL"/>

		<!--  -->
		<appender-ref ref="ASYNC_FILE_ERROR"/>

		<!--  -->
		<appender-ref ref="kafkaAppender"/>
	</root>

</configuration>